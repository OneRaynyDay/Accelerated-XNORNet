{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torch.nn.functional as F\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model import BinaryLinear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Doing 2d Pooling to reduce image size\n",
    "\n",
    "(we want 8x8's from 32x32's, so a 2d pool of 4x4 filters with 4 stride would work)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_image_avg(X, filter_size, stride_size):\n",
    "    \"\"\"\n",
    "    :param X: NxN image.\n",
    "    :param filter_size: the size of the filter e.g. (4x4) = 4\n",
    "    :param stride_size: the stride size\n",
    "    \"\"\"\n",
    "    tX = torch.from_numpy(X)\n",
    "    ty = torch.nn.MaxPool2d(kernel_size=filter_size, stride=stride_size)(tX)\n",
    "    y = ty.numpy()\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking binarized input data for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torchvision.datasets.MNIST(root='torch_dataset', \n",
    "                                          train=True, \n",
    "                                          transform=transforms.Compose([\n",
    "                                                              transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                                                          ]),\n",
    "                                          download=True)\n",
    "\n",
    "dataset = list(test_dataset)\n",
    "dataset = [x for x in dataset if x[1] < 5]\n",
    "# filter by only 0, 1, 2, 3, 4\n",
    "\n",
    "X = np.array([_x[0].numpy() for _x in dataset])\n",
    "y = np.array([_x[1] for _x in dataset])\n",
    "\n",
    "X = reduce_image_avg(X, 4, 4)\n",
    "\n",
    "X[X < 0.5] = 0\n",
    "X[X > 0.5] = 1\n",
    "X = X.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sanity Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYQQOFEbgQGEEDhRG4EBhBA4U9l8E67BG4/PeFQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label : 0\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4yLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvOIA7rQAACl5JREFUeJzt3e+rl/Udx/HXq5NpWS22XJjK7EYL\nIjYNcYwiNqOyFbUbu1FQsDHwzhrGBlG7M/oHot0YA1G3Rj8kKiGiZbKMFiyXmv3wRyHSSNewFlE2\nVtNeu3Eu28nZzqXnuq7z7d3zAQfP9/jt+3mrPc91fX+c78dJBKCmk6Z7AAD9IXCgMAIHCiNwoDAC\nBwojcKAwAgcKI3CgMAIHCju5jxs9xTMzS7P7uGkAkv6lD/RRPvRk1+sl8FmarW/58j5uGoCkzflj\nq+txig4URuBAYQQOFEbgQGEEDhRG4EBhBA4URuBAYQQOFEbgQGGtAre93PartvfYvr3voQB0Y9LA\nbY9J+rWkqyVdKOlG2xf2PRiAqWtzBF8qaU+SvUk+krRO0vX9jgWgC20CnyfpjQmX9zVfAzDiOvtx\nUdsrJK2QpFk6raubBTAFbY7g+yUtmHB5fvO1T0myKsmSJEtmaGZX8wGYgjaBPy/pfNvn2T5F0g2S\nHu13LABdmPQUPckh27dI2iBpTNLaJDt6nwzAlLW6D57kcUmP9zwLgI7xSjagMAIHCiNwoDACBwoj\ncKAwAgcKI3CgMAIHCiNwoDACBwrrZXfR6bThb9unbe2rzl00bWt/kfFv/tk4ggOFEThQGIEDhRE4\nUBiBA4UROFAYgQOFEThQGIEDhRE4UBiBA4W12V10re0Dtl8ZYiAA3WlzBP+dpOU9zwGgB5MGnuQZ\nSe8MMAuAjnEfHCiM7YOBwjo7grN9MDB6OEUHCmvzNNkDkv4s6QLb+2z/uP+xAHShzf7gNw4xCIDu\ncYoOFEbgQGEEDhRG4EBhBA4URuBAYQQOFEbgQGEEDhRG4EBh5bYPxvSYzi188dk4ggOFEThQGIED\nhRE4UBiBA4UROFAYgQOFEThQGIEDhRE4UBiBA4W1eV/0BbY32d5pe4ftlUMMBmDq2vywySFJP0+y\nzfYZkrba3phkZ8+zAZiiNtsHv5lkW/P5+5J2SZrX92AApu647oPbXihpsaTNfQwDoFutfx7c9umS\nHpZ0a5L3jvH7bB8MjJhWR3DbMzQe931JHjnWddg+GBg9bR5Ft6Q1knYluav/kQB0pc0R/BJJN0ta\nZnt78/G9nucC0IE22wc/K8kDzAKgY7ySDSiMwIHCCBwojMCBwggcKIzAgcIIHCiMwIHCCBwojMCB\nwnrZPvjr3/inNmz44m0nyxa6GDUcwYHCCBwojMCBwggcKIzAgcIIHCiMwIHCCBwojMCBwggcKIzA\ngcLabHwwy/ZfbL/YbB985xCDAZi6Nj9s8qGkZUkONlsYPWv7D0me63k2AFPUZuODSDrYXJzRfKTP\noQB0o+3mg2O2t0s6IGljErYPBj4HWgWe5HCSRZLmS1pq+6Kjr2N7he0ttre89Y/DXc8J4AQc16Po\nSd6VtEnS8mP83ifbB8/5ylhX8wGYgjaPos+xfVbz+amSrpC0u+/BAExdm0fR50q6x/aYxr8hPJjk\nsX7HAtCFNo+ivyRp8QCzAOgYr2QDCiNwoDACBwojcKAwAgcKI3CgMAIHCiNwoDACBwojcKAwAgcK\n62V/8NdeOk1Xnbuoj5ue1Bd1j+7p+vs+Yjr/3qf7zz7KOIIDhRE4UBiBA4UROFAYgQOFEThQGIED\nhRE4UBiBA4UROFBY68Cb/clesM17ogOfE8dzBF8paVdfgwDoXtvdRedLukbS6n7HAdCltkfwuyXd\nJunjHmcB0LE2mw9eK+lAkq2TXO+T7YP/rQ87GxDAiWtzBL9E0nW2X5e0TtIy2/cefaWJ2wfP0MyO\nxwRwIiYNPMkdSeYnWSjpBklPJbmp98kATBnPgwOFHddbNiV5WtLTvUwCoHMcwYHCCBwojMCBwggc\nKIzAgcIIHCiMwIHCCBwojMCBwggcKIzAgcJ62T54OrGVLPBfHMGBwggcKIzAgcIIHCiMwIHCCBwo\njMCBwggcKIzAgcIIHCiMwIHCWr0Wvdm26H1JhyUdSrKkz6EAdON4ftjku0ne7m0SAJ3jFB0orG3g\nkfSk7a22VxzrCmwfDIyetqfolybZb/urkjba3p3kmYlXSLJK0ipJOtNfTsdzAjgBrY7gSfY3vx6Q\ntF7S0j6HAtCNSQO3Pdv2GUc+l3SlpFf6HgzA1LU5RT9H0nrbR65/f5Inep0KQCcmDTzJXknfHGAW\nAB3jaTKgMAIHCiNwoDACBwojcKAwAgcKI3CgMAIHCiNwoDACBwojcKAwAgcKI3CgMAIHCiNwoDAC\nBwojcKAwAgcKI3CgMAIHCiNwoDACBwprFbjts2w/ZHu37V22v933YACmru3eZL+S9ESSH9g+RdJp\nPc4EoCOTBm77S5Iuk/RDSUrykaSP+h0LQBfanKKfJ+ktSb+1/YLt1c0eZZ/C9sHA6GkT+MmSLpb0\nmySLJX0g6fajr5RkVZIlSZbM0MyOxwRwItoEvk/SviSbm8sPaTx4ACNu0sCT/F3SG7YvaL50uaSd\nvU4FoBNtH0X/qaT7mkfQ90r6UX8jAehKq8CTbJe0pOdZAHSMV7IBhRE4UBiBA4UROFAYgQOFEThQ\nGIEDhRE4UBiBA4UROFCYk3R/o/Zbkv56gv/52ZLe7nAc1mbtimt/Lcmcya7US+BTYXtLkml53Ttr\ns3a1tTlFBwojcKCwUQx8FWuzNmt3Y+TugwPozigewQF0ZKQCt73c9qu299j+n3du7XHdtbYP2H5l\nqDUnrL3A9ibbO23vsL1ywLVn2f6L7Rebte8cau0JM4w1b8f92MDrvm77ZdvbbW8ZeO3BdgoamVN0\n22OSXpN0hcbfyfV5STcm6f0NHm1fJumgpN8nuajv9Y5ae66kuUm22T5D0lZJ3x/oz21Js5MctD1D\n0rOSViZ5ru+1J8zwM42/HdiZSa4dcN3XJS1JMvjz4LbvkfSnJKuP7BSU5N0+1hqlI/hSSXuS7G12\nT1kn6fohFk7yjKR3hljrGGu/mWRb8/n7knZJmjfQ2klysLk4o/kY7Du+7fmSrpG0eqg1p9uEnYLW\nSOM7BfUVtzRagc+T9MaEy/s00P/oo8L2QkmLJW3+/9fsdM0x29slHZC0ccL73w/hbkm3Sfp4wDWP\niKQnbW+1vWLAdVvtFNSVUQr8C8326ZIelnRrkveGWjfJ4SSLJM2XtNT2IHdRbF8r6UCSrUOsdwyX\nJrlY0tWSftLcTRtCq52CujJKge+XtGDC5fnN18pr7v8+LOm+JI9MxwzNaeImScsHWvISSdc194XX\nSVpm+96B1laS/c2vBySt1/hdxCEMulPQKAX+vKTzbZ/XPPBwg6RHp3mm3jUPdK2RtCvJXQOvPcf2\nWc3np2r8Ac7dQ6yd5I4k85Ms1Pi/9VNJbhpibduzmwc01ZweXylpkGdQht4pqO3OJr1Lcsj2LZI2\nSBqTtDbJjiHWtv2ApO9IOtv2Pkm/TLJmiLU1fiS7WdLLzX1hSfpFkscHWHuupHuaZzBOkvRgkkGf\nrpom50haP/69VSdLuj/JEwOuP9hOQSPzNBmA7o3SKTqAjhE4UBiBA4UROFAYgQOFEThQGIEDhRE4\nUNh/ANZJt6him4fRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label : 4\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPgAAAD8CAYAAABaQGkdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nCiNwoDACBwojcKCw/wG/S6j1gszvmQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label : 1\n"
     ]
    }
   ],
   "source": [
    "for i in range(3):\n",
    "    plt.imshow(X[i][0])\n",
    "    plt.show()\n",
    "    print('Label : {}'.format(y[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the matrix (We feed in the FPGA using flattened matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('matrices/bin_mnist_3d_tensor.npz', X)\n",
    "X = X.reshape(X.shape[0], -1)\n",
    "np.save('matrices/bin_mnist_flat.npz', X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pytorch preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.utils.data.TensorDataset(torch.from_numpy(X.astype(np.float32)), torch.from_numpy(y))\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to train a torch model on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 2 layer neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=5):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc = BinaryLinear(49, 5)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        return out\n",
    "\n",
    "model = Net().to(device)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "losses = []\n",
    "\n",
    "max_loss = float('inf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [50/957], Loss: 1.7428\n",
      "Test Accuracy of the model on the test images: 38.877631062884035 %\n",
      "Epoch [1/1], Step [100/957], Loss: 0.4911\n",
      "Test Accuracy of the model on the test images: 68.99921558373643 %\n",
      "Epoch [1/1], Step [150/957], Loss: 0.8834\n",
      "Test Accuracy of the model on the test images: 71.3982219898026 %\n",
      "Epoch [1/1], Step [200/957], Loss: 0.8392\n",
      "Test Accuracy of the model on the test images: 75.24186168126552 %\n",
      "Epoch [1/1], Step [250/957], Loss: 0.3316\n",
      "Test Accuracy of the model on the test images: 82.04994116878024 %\n",
      "Epoch [1/1], Step [300/957], Loss: 0.3459\n",
      "Epoch [1/1], Step [350/957], Loss: 0.6902\n",
      "Epoch [1/1], Step [400/957], Loss: 0.9732\n",
      "Epoch [1/1], Step [450/957], Loss: 0.9771\n",
      "Epoch [1/1], Step [500/957], Loss: 0.4339\n",
      "Epoch [1/1], Step [550/957], Loss: 0.4236\n",
      "Epoch [1/1], Step [600/957], Loss: 0.6645\n",
      "Epoch [1/1], Step [650/957], Loss: 0.5989\n",
      "Epoch [1/1], Step [700/957], Loss: 0.9015\n",
      "Epoch [1/1], Step [750/957], Loss: 0.3992\n",
      "Epoch [1/1], Step [800/957], Loss: 0.5972\n",
      "Test Accuracy of the model on the test images: 82.79186821806772 %\n",
      "Epoch [1/1], Step [850/957], Loss: 0.4285\n",
      "Epoch [1/1], Step [900/957], Loss: 1.4977\n",
      "Epoch [1/1], Step [950/957], Loss: 0.2480\n",
      "Epoch [2/1], Step [50/957], Loss: 0.6884\n",
      "Epoch [2/1], Step [100/957], Loss: 1.1579\n",
      "Epoch [2/1], Step [150/957], Loss: 2.4253\n",
      "Epoch [2/1], Step [200/957], Loss: 0.3862\n",
      "Epoch [2/1], Step [250/957], Loss: 0.6329\n",
      "Epoch [2/1], Step [300/957], Loss: 0.7564\n",
      "Epoch [2/1], Step [350/957], Loss: 1.0909\n",
      "Epoch [2/1], Step [400/957], Loss: 1.0993\n",
      "Epoch [2/1], Step [450/957], Loss: 0.6970\n",
      "Epoch [2/1], Step [500/957], Loss: 0.5309\n",
      "Epoch [2/1], Step [550/957], Loss: 0.4717\n",
      "Epoch [2/1], Step [600/957], Loss: 1.1481\n",
      "Epoch [2/1], Step [650/957], Loss: 0.3549\n",
      "Epoch [2/1], Step [700/957], Loss: 0.7488\n",
      "Epoch [2/1], Step [750/957], Loss: 1.1945\n",
      "Epoch [2/1], Step [800/957], Loss: 2.2169\n",
      "Epoch [2/1], Step [850/957], Loss: 1.0783\n",
      "Epoch [2/1], Step [900/957], Loss: 0.5592\n",
      "Epoch [2/1], Step [950/957], Loss: 0.3684\n",
      "Epoch [3/1], Step [50/957], Loss: 0.8081\n",
      "Epoch [3/1], Step [100/957], Loss: 0.6950\n",
      "Epoch [3/1], Step [150/957], Loss: 0.5195\n",
      "Epoch [3/1], Step [200/957], Loss: 0.6034\n",
      "Epoch [3/1], Step [250/957], Loss: 0.3423\n",
      "Epoch [3/1], Step [300/957], Loss: 0.4878\n",
      "Epoch [3/1], Step [350/957], Loss: 0.8304\n",
      "Test Accuracy of the model on the test images: 83.21676036083149 %\n",
      "Epoch [3/1], Step [400/957], Loss: 1.1058\n",
      "Epoch [3/1], Step [450/957], Loss: 0.5826\n",
      "Epoch [3/1], Step [500/957], Loss: 0.6913\n",
      "Test Accuracy of the model on the test images: 84.38031115178455 %\n",
      "Epoch [3/1], Step [550/957], Loss: 0.8101\n",
      "Epoch [3/1], Step [600/957], Loss: 0.6484\n",
      "Epoch [3/1], Step [650/957], Loss: 1.3821\n",
      "Epoch [3/1], Step [700/957], Loss: 1.7997\n",
      "Epoch [3/1], Step [750/957], Loss: 1.8837\n",
      "Epoch [3/1], Step [800/957], Loss: 0.3516\n",
      "Epoch [3/1], Step [850/957], Loss: 0.4549\n",
      "Epoch [3/1], Step [900/957], Loss: 0.9593\n",
      "Epoch [3/1], Step [950/957], Loss: 0.3865\n",
      "Epoch [4/1], Step [50/957], Loss: 0.5365\n",
      "Epoch [4/1], Step [100/957], Loss: 0.7014\n",
      "Epoch [4/1], Step [150/957], Loss: 0.5367\n",
      "Epoch [4/1], Step [200/957], Loss: 0.3524\n",
      "Epoch [4/1], Step [250/957], Loss: 0.8672\n",
      "Epoch [4/1], Step [300/957], Loss: 0.7319\n",
      "Epoch [4/1], Step [350/957], Loss: 0.4000\n",
      "Epoch [4/1], Step [400/957], Loss: 1.1214\n",
      "Epoch [4/1], Step [450/957], Loss: 0.6235\n",
      "Epoch [4/1], Step [500/957], Loss: 0.4459\n",
      "Epoch [4/1], Step [550/957], Loss: 0.8195\n",
      "Epoch [4/1], Step [600/957], Loss: 0.5068\n",
      "Epoch [4/1], Step [650/957], Loss: 0.9235\n",
      "Epoch [4/1], Step [700/957], Loss: 1.3347\n",
      "Epoch [4/1], Step [750/957], Loss: 0.7171\n",
      "Epoch [4/1], Step [800/957], Loss: 0.5868\n",
      "Epoch [4/1], Step [850/957], Loss: 0.6979\n",
      "Epoch [4/1], Step [900/957], Loss: 0.6890\n",
      "Epoch [4/1], Step [950/957], Loss: 1.1866\n",
      "Epoch [5/1], Step [50/957], Loss: 1.0781\n",
      "Epoch [5/1], Step [100/957], Loss: 1.2175\n",
      "Epoch [5/1], Step [150/957], Loss: 0.5642\n",
      "Epoch [5/1], Step [200/957], Loss: 1.0059\n",
      "Epoch [5/1], Step [250/957], Loss: 0.8096\n",
      "Epoch [5/1], Step [300/957], Loss: 0.2021\n",
      "Epoch [5/1], Step [350/957], Loss: 0.8708\n",
      "Epoch [5/1], Step [400/957], Loss: 0.6502\n",
      "Epoch [5/1], Step [450/957], Loss: 0.3615\n",
      "Epoch [5/1], Step [500/957], Loss: 1.1209\n",
      "Epoch [5/1], Step [550/957], Loss: 1.3181\n",
      "Epoch [5/1], Step [600/957], Loss: 0.5988\n",
      "Epoch [5/1], Step [650/957], Loss: 0.9632\n",
      "Epoch [5/1], Step [700/957], Loss: 0.8888\n",
      "Epoch [5/1], Step [750/957], Loss: 0.7425\n",
      "Epoch [5/1], Step [800/957], Loss: 0.6155\n",
      "Epoch [5/1], Step [850/957], Loss: 0.3729\n",
      "Epoch [5/1], Step [900/957], Loss: 1.4463\n",
      "Epoch [5/1], Step [950/957], Loss: 2.0313\n",
      "Epoch [6/1], Step [50/957], Loss: 0.5323\n",
      "Epoch [6/1], Step [100/957], Loss: 0.4245\n",
      "Epoch [6/1], Step [150/957], Loss: 0.7920\n",
      "Epoch [6/1], Step [200/957], Loss: 0.6612\n",
      "Epoch [6/1], Step [250/957], Loss: 0.7412\n",
      "Epoch [6/1], Step [300/957], Loss: 0.5387\n",
      "Epoch [6/1], Step [350/957], Loss: 0.8723\n",
      "Epoch [6/1], Step [400/957], Loss: 0.6211\n",
      "Epoch [6/1], Step [450/957], Loss: 0.4186\n",
      "Epoch [6/1], Step [500/957], Loss: 0.8476\n",
      "Epoch [6/1], Step [550/957], Loss: 0.6092\n",
      "Epoch [6/1], Step [600/957], Loss: 0.7989\n",
      "Epoch [6/1], Step [650/957], Loss: 0.4640\n",
      "Epoch [6/1], Step [700/957], Loss: 0.5474\n",
      "Epoch [6/1], Step [750/957], Loss: 1.1960\n",
      "Epoch [6/1], Step [800/957], Loss: 0.6104\n",
      "Epoch [6/1], Step [850/957], Loss: 1.8156\n",
      "Epoch [6/1], Step [900/957], Loss: 0.8319\n",
      "Epoch [6/1], Step [950/957], Loss: 0.4132\n",
      "Epoch [7/1], Step [50/957], Loss: 0.3800\n",
      "Epoch [7/1], Step [100/957], Loss: 0.8572\n",
      "Epoch [7/1], Step [150/957], Loss: 0.8785\n",
      "Epoch [7/1], Step [200/957], Loss: 1.3446\n",
      "Epoch [7/1], Step [250/957], Loss: 0.5659\n",
      "Epoch [7/1], Step [300/957], Loss: 0.6998\n",
      "Epoch [7/1], Step [350/957], Loss: 0.5958\n",
      "Epoch [7/1], Step [400/957], Loss: 0.6055\n",
      "Epoch [7/1], Step [450/957], Loss: 0.4984\n",
      "Epoch [7/1], Step [500/957], Loss: 0.6487\n",
      "Epoch [7/1], Step [550/957], Loss: 0.4152\n",
      "Epoch [7/1], Step [600/957], Loss: 1.0366\n",
      "Epoch [7/1], Step [650/957], Loss: 0.9041\n",
      "Epoch [7/1], Step [700/957], Loss: 0.6540\n",
      "Epoch [7/1], Step [750/957], Loss: 0.7823\n",
      "Epoch [7/1], Step [800/957], Loss: 1.2590\n",
      "Epoch [7/1], Step [850/957], Loss: 0.5804\n",
      "Epoch [7/1], Step [900/957], Loss: 0.4436\n",
      "Epoch [7/1], Step [950/957], Loss: 0.5442\n",
      "Epoch [8/1], Step [50/957], Loss: 0.8007\n",
      "Epoch [8/1], Step [100/957], Loss: 1.0424\n",
      "Epoch [8/1], Step [150/957], Loss: 1.3127\n",
      "Epoch [8/1], Step [200/957], Loss: 0.9425\n",
      "Epoch [8/1], Step [250/957], Loss: 0.8064\n",
      "Epoch [8/1], Step [300/957], Loss: 0.5171\n",
      "Epoch [8/1], Step [350/957], Loss: 0.9704\n",
      "Epoch [8/1], Step [400/957], Loss: 1.3667\n",
      "Epoch [8/1], Step [450/957], Loss: 0.8310\n",
      "Epoch [8/1], Step [500/957], Loss: 0.3928\n",
      "Epoch [8/1], Step [550/957], Loss: 0.5833\n",
      "Epoch [8/1], Step [600/957], Loss: 0.7976\n",
      "Epoch [8/1], Step [650/957], Loss: 0.5995\n",
      "Epoch [8/1], Step [700/957], Loss: 1.3139\n",
      "Epoch [8/1], Step [750/957], Loss: 0.8662\n",
      "Epoch [8/1], Step [800/957], Loss: 0.7415\n",
      "Epoch [8/1], Step [850/957], Loss: 0.9987\n",
      "Epoch [8/1], Step [900/957], Loss: 0.9799\n",
      "Epoch [8/1], Step [950/957], Loss: 0.5279\n",
      "Epoch [9/1], Step [50/957], Loss: 0.5479\n",
      "Epoch [9/1], Step [100/957], Loss: 0.6602\n",
      "Epoch [9/1], Step [150/957], Loss: 0.7414\n",
      "Epoch [9/1], Step [200/957], Loss: 1.2777\n",
      "Epoch [9/1], Step [250/957], Loss: 1.3927\n",
      "Epoch [9/1], Step [300/957], Loss: 2.2285\n",
      "Epoch [9/1], Step [350/957], Loss: 0.6556\n",
      "Epoch [9/1], Step [400/957], Loss: 1.0336\n",
      "Epoch [9/1], Step [450/957], Loss: 0.3988\n",
      "Epoch [9/1], Step [500/957], Loss: 0.8614\n",
      "Epoch [9/1], Step [550/957], Loss: 0.5881\n",
      "Epoch [9/1], Step [600/957], Loss: 0.3436\n",
      "Epoch [9/1], Step [650/957], Loss: 0.7389\n",
      "Epoch [9/1], Step [700/957], Loss: 0.3988\n",
      "Epoch [9/1], Step [750/957], Loss: 0.6291\n",
      "Epoch [9/1], Step [800/957], Loss: 1.2311\n",
      "Epoch [9/1], Step [850/957], Loss: 1.1335\n",
      "Epoch [9/1], Step [900/957], Loss: 1.3093\n",
      "Epoch [9/1], Step [950/957], Loss: 0.5932\n",
      "Epoch [10/1], Step [50/957], Loss: 1.3930\n",
      "Epoch [10/1], Step [100/957], Loss: 0.4996\n",
      "Epoch [10/1], Step [150/957], Loss: 0.8339\n",
      "Epoch [10/1], Step [200/957], Loss: 0.4656\n",
      "Epoch [10/1], Step [250/957], Loss: 1.0484\n",
      "Epoch [10/1], Step [300/957], Loss: 1.0681\n",
      "Epoch [10/1], Step [350/957], Loss: 0.7118\n",
      "Epoch [10/1], Step [400/957], Loss: 1.0581\n",
      "Epoch [10/1], Step [450/957], Loss: 0.6069\n",
      "Epoch [10/1], Step [500/957], Loss: 1.3616\n",
      "Epoch [10/1], Step [550/957], Loss: 1.3852\n",
      "Epoch [10/1], Step [600/957], Loss: 0.5315\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [10/1], Step [650/957], Loss: 1.4762\n",
      "Epoch [10/1], Step [700/957], Loss: 0.8864\n",
      "Epoch [10/1], Step [750/957], Loss: 0.9452\n",
      "Epoch [10/1], Step [800/957], Loss: 0.6464\n",
      "Epoch [10/1], Step [850/957], Loss: 0.4957\n",
      "Epoch [10/1], Step [900/957], Loss: 0.8519\n",
      "Epoch [10/1], Step [950/957], Loss: 0.9439\n"
     ]
    }
   ],
   "source": [
    "def eval_model(model, train_loader, device):\n",
    "    model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "    with torch.no_grad():\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        for images, labels in train_loader:\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "        acc = 100 * correct / total\n",
    "        return acc\n",
    "\n",
    "max_acc = 0\n",
    "    \n",
    "for epoch in range(10):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 50 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, 1, i+1, total_step, loss.item()))\n",
    "            cur_acc = eval_model(model, train_loader, device)\n",
    "            if cur_acc > max_acc:\n",
    "                max_acc = cur_acc\n",
    "                print('Test Accuracy of the model on the test images: {} %'.format(cur_acc))\n",
    "                torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model.ckpt'))\n",
    "params = dict(model.fc.named_parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To actually retrieve the values, we take the magnitude and the signs:\n",
    "for pname in params:\n",
    "    w = params[pname].data.cpu().numpy()\n",
    "    sign = np.sign(w)\n",
    "    sign[sign < 0] = 0\n",
    "    sign = sign.astype(np.int8)\n",
    "    np.save('{}'.format(pname), sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "w = np.load('weight.npy').astype(int)\n",
    "x = X.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8357 10000\n",
      "0.839717610145117\n"
     ]
    }
   ],
   "source": [
    "# Mimicking the FPGA's behavior for XNORNET:\n",
    "def predict(data, weights):\n",
    "    # Now try to classify the 4:\n",
    "    idx_max = 0\n",
    "    max_val = 0\n",
    "    for class_idx, row in enumerate(weights):\n",
    "        rx = data\n",
    "        s = 0\n",
    "        for a,b in zip(row, rx):\n",
    "            s += a & b # (1-(a ^ b))\n",
    "        if s > max_val:\n",
    "            max_val = s\n",
    "            idx_max = class_idx\n",
    "    return idx_max\n",
    "\n",
    "corrects = 0\n",
    "n = 10000\n",
    "for i in range(n):\n",
    "    pred = predict(x[i], w)\n",
    "    corrects += pred == y[i]\n",
    "print(corrects, n)\n",
    "\n",
    "print(np.mean(np.argmax(x.dot(w.T), axis=1) == y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
