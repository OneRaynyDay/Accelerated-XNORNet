{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from model import BinaryLinear"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Taking binarized input data for MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = torchvision.datasets.MNIST(root='torch_dataset', \n",
    "                                          train=True, \n",
    "                                          transform=transforms.Compose([\n",
    "                                                              transforms.ToTensor(), # first, convert image to PyTorch tensor\n",
    "                                                          ]),\n",
    "                                          download=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = list(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([_x[0].numpy() for _x in dataset])\n",
    "y = np.array([_x[1] for _x in dataset])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X[X < 0.5] = 0\n",
    "X[X > 0.5] = 1\n",
    "X = X.astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('bin_mnist_3d_tensor.npz', X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.reshape(60000, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('bin_mnist_flat.npz', X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt('bin_mnist_flat.csv', X, fmt='%i', delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.utils.data.TensorDataset(torch.from_numpy(X.astype(np.float32)), torch.from_numpy(y))\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Try to train a torch model on it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# 2 layer neural network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_classes=10):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            BinaryLinear(28*28, 128),\n",
    "            nn.ReLU(),\n",
    "            BinaryLinear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            BinaryLinear(64, num_classes))\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.fc(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1], Step [50/1875], Loss: 2.3025\n",
      "Epoch [1/1], Step [100/1875], Loss: 2.3050\n",
      "Epoch [1/1], Step [150/1875], Loss: 2.2385\n",
      "Epoch [1/1], Step [200/1875], Loss: 2.1703\n",
      "Epoch [1/1], Step [250/1875], Loss: 1.7583\n",
      "Epoch [1/1], Step [300/1875], Loss: 1.3345\n",
      "Epoch [1/1], Step [350/1875], Loss: 1.2034\n",
      "Epoch [1/1], Step [400/1875], Loss: 1.2178\n",
      "Epoch [1/1], Step [450/1875], Loss: 1.0049\n",
      "Epoch [1/1], Step [500/1875], Loss: 1.0994\n",
      "Epoch [1/1], Step [550/1875], Loss: 0.7945\n",
      "Epoch [1/1], Step [600/1875], Loss: 0.5994\n",
      "Epoch [1/1], Step [650/1875], Loss: 0.3887\n",
      "Epoch [1/1], Step [700/1875], Loss: 0.6215\n",
      "Epoch [1/1], Step [750/1875], Loss: 0.4411\n",
      "Epoch [1/1], Step [800/1875], Loss: 0.3505\n",
      "Epoch [1/1], Step [850/1875], Loss: 0.3562\n",
      "Epoch [1/1], Step [900/1875], Loss: 0.3850\n",
      "Epoch [1/1], Step [950/1875], Loss: 0.4271\n",
      "Epoch [1/1], Step [1000/1875], Loss: 0.4200\n",
      "Epoch [1/1], Step [1050/1875], Loss: 0.6697\n",
      "Epoch [1/1], Step [1100/1875], Loss: 0.5931\n",
      "Epoch [1/1], Step [1150/1875], Loss: 0.4249\n",
      "Epoch [1/1], Step [1200/1875], Loss: 0.1738\n",
      "Epoch [1/1], Step [1250/1875], Loss: 0.6164\n",
      "Epoch [1/1], Step [1300/1875], Loss: 0.4935\n",
      "Epoch [1/1], Step [1350/1875], Loss: 0.2930\n",
      "Epoch [1/1], Step [1400/1875], Loss: 0.4079\n",
      "Epoch [1/1], Step [1450/1875], Loss: 0.3971\n",
      "Epoch [1/1], Step [1500/1875], Loss: 0.6757\n",
      "Epoch [1/1], Step [1550/1875], Loss: 0.5269\n",
      "Epoch [1/1], Step [1600/1875], Loss: 0.6444\n",
      "Epoch [1/1], Step [1650/1875], Loss: 0.2196\n",
      "Epoch [1/1], Step [1700/1875], Loss: 0.7347\n",
      "Epoch [1/1], Step [1750/1875], Loss: 0.5363\n",
      "Epoch [1/1], Step [1800/1875], Loss: 0.2476\n",
      "Epoch [1/1], Step [1850/1875], Loss: 0.2039\n",
      "Epoch [2/1], Step [50/1875], Loss: 0.2632\n",
      "Epoch [2/1], Step [100/1875], Loss: 0.3971\n",
      "Epoch [2/1], Step [150/1875], Loss: 0.3056\n",
      "Epoch [2/1], Step [200/1875], Loss: 0.3506\n",
      "Epoch [2/1], Step [250/1875], Loss: 0.2799\n",
      "Epoch [2/1], Step [300/1875], Loss: 0.2646\n",
      "Epoch [2/1], Step [350/1875], Loss: 0.4423\n",
      "Epoch [2/1], Step [400/1875], Loss: 0.3654\n",
      "Epoch [2/1], Step [450/1875], Loss: 0.1502\n",
      "Epoch [2/1], Step [500/1875], Loss: 0.4474\n",
      "Epoch [2/1], Step [550/1875], Loss: 0.0962\n",
      "Epoch [2/1], Step [600/1875], Loss: 0.5774\n",
      "Epoch [2/1], Step [650/1875], Loss: 0.2087\n",
      "Epoch [2/1], Step [700/1875], Loss: 0.1759\n",
      "Epoch [2/1], Step [750/1875], Loss: 0.2313\n",
      "Epoch [2/1], Step [800/1875], Loss: 0.2124\n",
      "Epoch [2/1], Step [850/1875], Loss: 0.4525\n",
      "Epoch [2/1], Step [900/1875], Loss: 0.5387\n",
      "Epoch [2/1], Step [950/1875], Loss: 0.3338\n",
      "Epoch [2/1], Step [1000/1875], Loss: 0.5541\n",
      "Epoch [2/1], Step [1050/1875], Loss: 0.1123\n",
      "Epoch [2/1], Step [1100/1875], Loss: 0.3241\n",
      "Epoch [2/1], Step [1150/1875], Loss: 0.2932\n",
      "Epoch [2/1], Step [1200/1875], Loss: 0.1986\n",
      "Epoch [2/1], Step [1250/1875], Loss: 0.1669\n",
      "Epoch [2/1], Step [1300/1875], Loss: 0.2660\n",
      "Epoch [2/1], Step [1350/1875], Loss: 0.3610\n",
      "Epoch [2/1], Step [1400/1875], Loss: 0.3136\n",
      "Epoch [2/1], Step [1450/1875], Loss: 0.0885\n",
      "Epoch [2/1], Step [1500/1875], Loss: 0.3463\n",
      "Epoch [2/1], Step [1550/1875], Loss: 0.2043\n",
      "Epoch [2/1], Step [1600/1875], Loss: 0.3920\n",
      "Epoch [2/1], Step [1650/1875], Loss: 0.2968\n",
      "Epoch [2/1], Step [1700/1875], Loss: 0.1817\n",
      "Epoch [2/1], Step [1750/1875], Loss: 0.2614\n",
      "Epoch [2/1], Step [1800/1875], Loss: 0.1952\n",
      "Epoch [2/1], Step [1850/1875], Loss: 0.2896\n",
      "Epoch [3/1], Step [50/1875], Loss: 0.2686\n",
      "Epoch [3/1], Step [100/1875], Loss: 0.5763\n",
      "Epoch [3/1], Step [150/1875], Loss: 0.2101\n",
      "Epoch [3/1], Step [200/1875], Loss: 0.3370\n",
      "Epoch [3/1], Step [250/1875], Loss: 0.1165\n",
      "Epoch [3/1], Step [300/1875], Loss: 0.3802\n",
      "Epoch [3/1], Step [350/1875], Loss: 0.1342\n",
      "Epoch [3/1], Step [400/1875], Loss: 0.3201\n",
      "Epoch [3/1], Step [450/1875], Loss: 0.2037\n",
      "Epoch [3/1], Step [500/1875], Loss: 0.4076\n",
      "Epoch [3/1], Step [550/1875], Loss: 0.1619\n",
      "Epoch [3/1], Step [600/1875], Loss: 0.2577\n",
      "Epoch [3/1], Step [650/1875], Loss: 0.3157\n",
      "Epoch [3/1], Step [700/1875], Loss: 0.2169\n",
      "Epoch [3/1], Step [750/1875], Loss: 0.2624\n",
      "Epoch [3/1], Step [800/1875], Loss: 0.0894\n",
      "Epoch [3/1], Step [850/1875], Loss: 0.2143\n",
      "Epoch [3/1], Step [900/1875], Loss: 0.4266\n",
      "Epoch [3/1], Step [950/1875], Loss: 0.0766\n",
      "Epoch [3/1], Step [1000/1875], Loss: 0.2191\n",
      "Epoch [3/1], Step [1050/1875], Loss: 0.2318\n",
      "Epoch [3/1], Step [1100/1875], Loss: 0.2078\n",
      "Epoch [3/1], Step [1150/1875], Loss: 0.3158\n",
      "Epoch [3/1], Step [1200/1875], Loss: 0.2135\n",
      "Epoch [3/1], Step [1250/1875], Loss: 0.2003\n",
      "Epoch [3/1], Step [1300/1875], Loss: 0.0186\n",
      "Epoch [3/1], Step [1350/1875], Loss: 0.2927\n",
      "Epoch [3/1], Step [1400/1875], Loss: 0.3811\n",
      "Epoch [3/1], Step [1450/1875], Loss: 0.3975\n",
      "Epoch [3/1], Step [1500/1875], Loss: 0.4164\n",
      "Epoch [3/1], Step [1550/1875], Loss: 0.5284\n",
      "Epoch [3/1], Step [1600/1875], Loss: 0.4204\n",
      "Epoch [3/1], Step [1650/1875], Loss: 0.1512\n",
      "Epoch [3/1], Step [1700/1875], Loss: 0.1027\n",
      "Epoch [3/1], Step [1750/1875], Loss: 0.1959\n",
      "Epoch [3/1], Step [1800/1875], Loss: 0.5057\n",
      "Epoch [3/1], Step [1850/1875], Loss: 0.2691\n",
      "Epoch [4/1], Step [50/1875], Loss: 0.2800\n",
      "Epoch [4/1], Step [100/1875], Loss: 0.1891\n",
      "Epoch [4/1], Step [150/1875], Loss: 0.1923\n",
      "Epoch [4/1], Step [200/1875], Loss: 0.2455\n",
      "Epoch [4/1], Step [250/1875], Loss: 0.2687\n",
      "Epoch [4/1], Step [300/1875], Loss: 0.3077\n",
      "Epoch [4/1], Step [350/1875], Loss: 0.1773\n",
      "Epoch [4/1], Step [400/1875], Loss: 0.2645\n",
      "Epoch [4/1], Step [450/1875], Loss: 0.3109\n",
      "Epoch [4/1], Step [500/1875], Loss: 0.1138\n",
      "Epoch [4/1], Step [550/1875], Loss: 0.1178\n",
      "Epoch [4/1], Step [600/1875], Loss: 0.2421\n",
      "Epoch [4/1], Step [650/1875], Loss: 0.1999\n",
      "Epoch [4/1], Step [700/1875], Loss: 0.3671\n",
      "Epoch [4/1], Step [750/1875], Loss: 0.3031\n",
      "Epoch [4/1], Step [800/1875], Loss: 0.2982\n",
      "Epoch [4/1], Step [850/1875], Loss: 0.1154\n",
      "Epoch [4/1], Step [900/1875], Loss: 0.0928\n",
      "Epoch [4/1], Step [950/1875], Loss: 0.1518\n",
      "Epoch [4/1], Step [1000/1875], Loss: 0.1975\n",
      "Epoch [4/1], Step [1050/1875], Loss: 0.3547\n",
      "Epoch [4/1], Step [1100/1875], Loss: 0.4931\n",
      "Epoch [4/1], Step [1150/1875], Loss: 0.3191\n",
      "Epoch [4/1], Step [1200/1875], Loss: 0.3123\n",
      "Epoch [4/1], Step [1250/1875], Loss: 0.2472\n",
      "Epoch [4/1], Step [1300/1875], Loss: 0.0803\n",
      "Epoch [4/1], Step [1350/1875], Loss: 0.2766\n",
      "Epoch [4/1], Step [1400/1875], Loss: 0.0693\n",
      "Epoch [4/1], Step [1450/1875], Loss: 0.1470\n",
      "Epoch [4/1], Step [1500/1875], Loss: 0.1918\n",
      "Epoch [4/1], Step [1550/1875], Loss: 0.2672\n",
      "Epoch [4/1], Step [1600/1875], Loss: 0.4469\n",
      "Epoch [4/1], Step [1650/1875], Loss: 0.2282\n",
      "Epoch [4/1], Step [1700/1875], Loss: 0.0347\n",
      "Epoch [4/1], Step [1750/1875], Loss: 0.4531\n",
      "Epoch [4/1], Step [1800/1875], Loss: 0.3080\n",
      "Epoch [4/1], Step [1850/1875], Loss: 0.1425\n",
      "Epoch [5/1], Step [50/1875], Loss: 0.3897\n",
      "Epoch [5/1], Step [100/1875], Loss: 0.1209\n",
      "Epoch [5/1], Step [150/1875], Loss: 0.3619\n",
      "Epoch [5/1], Step [200/1875], Loss: 0.2296\n",
      "Epoch [5/1], Step [250/1875], Loss: 0.5917\n",
      "Epoch [5/1], Step [300/1875], Loss: 0.3181\n",
      "Epoch [5/1], Step [350/1875], Loss: 0.3122\n",
      "Epoch [5/1], Step [400/1875], Loss: 0.2131\n",
      "Epoch [5/1], Step [450/1875], Loss: 0.4705\n",
      "Epoch [5/1], Step [500/1875], Loss: 0.2057\n",
      "Epoch [5/1], Step [550/1875], Loss: 0.4885\n",
      "Epoch [5/1], Step [600/1875], Loss: 0.4264\n",
      "Epoch [5/1], Step [650/1875], Loss: 0.3495\n",
      "Epoch [5/1], Step [700/1875], Loss: 0.2212\n",
      "Epoch [5/1], Step [750/1875], Loss: 0.0622\n",
      "Epoch [5/1], Step [800/1875], Loss: 0.1786\n",
      "Epoch [5/1], Step [850/1875], Loss: 0.2340\n",
      "Epoch [5/1], Step [900/1875], Loss: 0.2093\n",
      "Epoch [5/1], Step [950/1875], Loss: 0.0784\n",
      "Epoch [5/1], Step [1000/1875], Loss: 0.1942\n",
      "Epoch [5/1], Step [1050/1875], Loss: 0.1440\n",
      "Epoch [5/1], Step [1100/1875], Loss: 0.2987\n",
      "Epoch [5/1], Step [1150/1875], Loss: 0.1750\n",
      "Epoch [5/1], Step [1200/1875], Loss: 0.1143\n",
      "Epoch [5/1], Step [1250/1875], Loss: 0.1719\n",
      "Epoch [5/1], Step [1300/1875], Loss: 0.1081\n",
      "Epoch [5/1], Step [1350/1875], Loss: 0.1749\n",
      "Epoch [5/1], Step [1400/1875], Loss: 0.4400\n",
      "Epoch [5/1], Step [1450/1875], Loss: 0.4401\n",
      "Epoch [5/1], Step [1500/1875], Loss: 0.3335\n",
      "Epoch [5/1], Step [1550/1875], Loss: 0.2912\n",
      "Epoch [5/1], Step [1600/1875], Loss: 0.0800\n",
      "Epoch [5/1], Step [1650/1875], Loss: 0.0733\n",
      "Epoch [5/1], Step [1700/1875], Loss: 0.1017\n",
      "Epoch [5/1], Step [1750/1875], Loss: 0.2035\n",
      "Epoch [5/1], Step [1800/1875], Loss: 0.2063\n",
      "Epoch [5/1], Step [1850/1875], Loss: 0.1683\n",
      "Epoch [6/1], Step [50/1875], Loss: 0.3692\n",
      "Epoch [6/1], Step [100/1875], Loss: 0.3141\n",
      "Epoch [6/1], Step [150/1875], Loss: 0.3464\n",
      "Epoch [6/1], Step [200/1875], Loss: 0.0309\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6/1], Step [250/1875], Loss: 0.3734\n",
      "Epoch [6/1], Step [300/1875], Loss: 0.1757\n",
      "Epoch [6/1], Step [350/1875], Loss: 0.1315\n",
      "Epoch [6/1], Step [400/1875], Loss: 0.2029\n",
      "Epoch [6/1], Step [450/1875], Loss: 0.3972\n",
      "Epoch [6/1], Step [500/1875], Loss: 0.1979\n",
      "Epoch [6/1], Step [550/1875], Loss: 0.0960\n",
      "Epoch [6/1], Step [600/1875], Loss: 0.3042\n",
      "Epoch [6/1], Step [650/1875], Loss: 0.5213\n",
      "Epoch [6/1], Step [700/1875], Loss: 0.0786\n",
      "Epoch [6/1], Step [750/1875], Loss: 0.4435\n",
      "Epoch [6/1], Step [800/1875], Loss: 0.2376\n",
      "Epoch [6/1], Step [850/1875], Loss: 0.2973\n",
      "Epoch [6/1], Step [900/1875], Loss: 0.2291\n",
      "Epoch [6/1], Step [950/1875], Loss: 0.2222\n",
      "Epoch [6/1], Step [1000/1875], Loss: 0.3481\n",
      "Epoch [6/1], Step [1050/1875], Loss: 0.1654\n",
      "Epoch [6/1], Step [1100/1875], Loss: 0.1456\n",
      "Epoch [6/1], Step [1150/1875], Loss: 0.2093\n",
      "Epoch [6/1], Step [1200/1875], Loss: 0.3843\n",
      "Epoch [6/1], Step [1250/1875], Loss: 0.2160\n",
      "Epoch [6/1], Step [1300/1875], Loss: 0.4836\n",
      "Epoch [6/1], Step [1350/1875], Loss: 0.1395\n",
      "Epoch [6/1], Step [1400/1875], Loss: 0.5657\n",
      "Epoch [6/1], Step [1450/1875], Loss: 0.3179\n",
      "Epoch [6/1], Step [1500/1875], Loss: 0.2541\n",
      "Epoch [6/1], Step [1550/1875], Loss: 0.3860\n",
      "Epoch [6/1], Step [1600/1875], Loss: 0.2548\n",
      "Epoch [6/1], Step [1650/1875], Loss: 0.3363\n",
      "Epoch [6/1], Step [1700/1875], Loss: 0.1822\n",
      "Epoch [6/1], Step [1750/1875], Loss: 0.1618\n",
      "Epoch [6/1], Step [1800/1875], Loss: 0.5913\n",
      "Epoch [6/1], Step [1850/1875], Loss: 0.1590\n",
      "Epoch [7/1], Step [50/1875], Loss: 0.1954\n",
      "Epoch [7/1], Step [100/1875], Loss: 0.2900\n",
      "Epoch [7/1], Step [150/1875], Loss: 0.1443\n",
      "Epoch [7/1], Step [200/1875], Loss: 0.0635\n",
      "Epoch [7/1], Step [250/1875], Loss: 0.2195\n",
      "Epoch [7/1], Step [300/1875], Loss: 0.1079\n",
      "Epoch [7/1], Step [350/1875], Loss: 0.3446\n",
      "Epoch [7/1], Step [400/1875], Loss: 0.3848\n",
      "Epoch [7/1], Step [450/1875], Loss: 0.1797\n",
      "Epoch [7/1], Step [500/1875], Loss: 0.2346\n",
      "Epoch [7/1], Step [550/1875], Loss: 0.2570\n",
      "Epoch [7/1], Step [600/1875], Loss: 0.0716\n",
      "Epoch [7/1], Step [650/1875], Loss: 0.2304\n",
      "Epoch [7/1], Step [700/1875], Loss: 0.1854\n",
      "Epoch [7/1], Step [750/1875], Loss: 0.4390\n",
      "Epoch [7/1], Step [800/1875], Loss: 0.2037\n",
      "Epoch [7/1], Step [850/1875], Loss: 0.1200\n",
      "Epoch [7/1], Step [900/1875], Loss: 0.1729\n",
      "Epoch [7/1], Step [950/1875], Loss: 0.2795\n",
      "Epoch [7/1], Step [1000/1875], Loss: 0.2016\n",
      "Epoch [7/1], Step [1050/1875], Loss: 0.1103\n",
      "Epoch [7/1], Step [1100/1875], Loss: 0.2782\n",
      "Epoch [7/1], Step [1150/1875], Loss: 0.3335\n",
      "Epoch [7/1], Step [1200/1875], Loss: 0.1699\n",
      "Epoch [7/1], Step [1250/1875], Loss: 0.1873\n",
      "Epoch [7/1], Step [1300/1875], Loss: 0.5423\n",
      "Epoch [7/1], Step [1350/1875], Loss: 0.1257\n",
      "Epoch [7/1], Step [1400/1875], Loss: 0.3154\n",
      "Epoch [7/1], Step [1450/1875], Loss: 0.2270\n",
      "Epoch [7/1], Step [1500/1875], Loss: 0.2161\n",
      "Epoch [7/1], Step [1550/1875], Loss: 0.3377\n",
      "Epoch [7/1], Step [1600/1875], Loss: 0.3912\n",
      "Epoch [7/1], Step [1650/1875], Loss: 0.3515\n",
      "Epoch [7/1], Step [1700/1875], Loss: 0.2756\n",
      "Epoch [7/1], Step [1750/1875], Loss: 0.2069\n",
      "Epoch [7/1], Step [1800/1875], Loss: 0.1523\n",
      "Epoch [7/1], Step [1850/1875], Loss: 0.1192\n",
      "Epoch [8/1], Step [50/1875], Loss: 0.4688\n",
      "Epoch [8/1], Step [100/1875], Loss: 0.1113\n",
      "Epoch [8/1], Step [150/1875], Loss: 0.1851\n",
      "Epoch [8/1], Step [200/1875], Loss: 0.3023\n",
      "Epoch [8/1], Step [250/1875], Loss: 0.1434\n",
      "Epoch [8/1], Step [300/1875], Loss: 0.1103\n",
      "Epoch [8/1], Step [350/1875], Loss: 0.2597\n",
      "Epoch [8/1], Step [400/1875], Loss: 0.4862\n",
      "Epoch [8/1], Step [450/1875], Loss: 0.3185\n",
      "Epoch [8/1], Step [500/1875], Loss: 0.4404\n",
      "Epoch [8/1], Step [550/1875], Loss: 0.0430\n",
      "Epoch [8/1], Step [600/1875], Loss: 0.1526\n",
      "Epoch [8/1], Step [650/1875], Loss: 0.0525\n",
      "Epoch [8/1], Step [700/1875], Loss: 0.2312\n",
      "Epoch [8/1], Step [750/1875], Loss: 0.0668\n",
      "Epoch [8/1], Step [800/1875], Loss: 0.1053\n",
      "Epoch [8/1], Step [850/1875], Loss: 0.2136\n",
      "Epoch [8/1], Step [900/1875], Loss: 0.1503\n",
      "Epoch [8/1], Step [950/1875], Loss: 0.3569\n",
      "Epoch [8/1], Step [1000/1875], Loss: 0.1114\n",
      "Epoch [8/1], Step [1050/1875], Loss: 0.1687\n",
      "Epoch [8/1], Step [1100/1875], Loss: 0.0657\n",
      "Epoch [8/1], Step [1150/1875], Loss: 0.2219\n",
      "Epoch [8/1], Step [1200/1875], Loss: 0.2326\n",
      "Epoch [8/1], Step [1250/1875], Loss: 0.4820\n",
      "Epoch [8/1], Step [1300/1875], Loss: 0.3739\n",
      "Epoch [8/1], Step [1350/1875], Loss: 0.4402\n",
      "Epoch [8/1], Step [1400/1875], Loss: 0.1339\n",
      "Epoch [8/1], Step [1450/1875], Loss: 0.1320\n",
      "Epoch [8/1], Step [1500/1875], Loss: 0.3753\n",
      "Epoch [8/1], Step [1550/1875], Loss: 0.0521\n",
      "Epoch [8/1], Step [1600/1875], Loss: 0.2686\n",
      "Epoch [8/1], Step [1650/1875], Loss: 0.1585\n",
      "Epoch [8/1], Step [1700/1875], Loss: 0.0519\n",
      "Epoch [8/1], Step [1750/1875], Loss: 0.3787\n",
      "Epoch [8/1], Step [1800/1875], Loss: 0.3257\n",
      "Epoch [8/1], Step [1850/1875], Loss: 0.1509\n",
      "Epoch [9/1], Step [50/1875], Loss: 0.2183\n",
      "Epoch [9/1], Step [100/1875], Loss: 0.7801\n",
      "Epoch [9/1], Step [150/1875], Loss: 0.2712\n",
      "Epoch [9/1], Step [200/1875], Loss: 0.1358\n",
      "Epoch [9/1], Step [250/1875], Loss: 0.2530\n",
      "Epoch [9/1], Step [300/1875], Loss: 0.2371\n",
      "Epoch [9/1], Step [350/1875], Loss: 0.4766\n",
      "Epoch [9/1], Step [400/1875], Loss: 0.1426\n",
      "Epoch [9/1], Step [450/1875], Loss: 0.1884\n",
      "Epoch [9/1], Step [500/1875], Loss: 0.4316\n",
      "Epoch [9/1], Step [550/1875], Loss: 0.1480\n",
      "Epoch [9/1], Step [600/1875], Loss: 0.1709\n",
      "Epoch [9/1], Step [650/1875], Loss: 0.1244\n",
      "Epoch [9/1], Step [700/1875], Loss: 0.1540\n",
      "Epoch [9/1], Step [750/1875], Loss: 0.1004\n",
      "Epoch [9/1], Step [800/1875], Loss: 0.3221\n",
      "Epoch [9/1], Step [850/1875], Loss: 0.0829\n",
      "Epoch [9/1], Step [900/1875], Loss: 0.2132\n",
      "Epoch [9/1], Step [950/1875], Loss: 0.2193\n",
      "Epoch [9/1], Step [1000/1875], Loss: 0.3065\n",
      "Epoch [9/1], Step [1050/1875], Loss: 0.1248\n",
      "Epoch [9/1], Step [1100/1875], Loss: 0.2569\n",
      "Epoch [9/1], Step [1150/1875], Loss: 0.2593\n",
      "Epoch [9/1], Step [1200/1875], Loss: 0.1380\n",
      "Epoch [9/1], Step [1250/1875], Loss: 0.2373\n",
      "Epoch [9/1], Step [1300/1875], Loss: 0.2946\n",
      "Epoch [9/1], Step [1350/1875], Loss: 0.2594\n",
      "Epoch [9/1], Step [1400/1875], Loss: 0.1924\n",
      "Epoch [9/1], Step [1450/1875], Loss: 0.2840\n",
      "Epoch [9/1], Step [1500/1875], Loss: 0.2818\n",
      "Epoch [9/1], Step [1550/1875], Loss: 0.3559\n",
      "Epoch [9/1], Step [1600/1875], Loss: 0.4777\n",
      "Epoch [9/1], Step [1650/1875], Loss: 0.1344\n",
      "Epoch [9/1], Step [1700/1875], Loss: 0.2058\n",
      "Epoch [9/1], Step [1750/1875], Loss: 0.3979\n",
      "Epoch [9/1], Step [1800/1875], Loss: 0.2290\n",
      "Epoch [9/1], Step [1850/1875], Loss: 0.4127\n",
      "Epoch [10/1], Step [50/1875], Loss: 0.2946\n",
      "Epoch [10/1], Step [100/1875], Loss: 0.1551\n",
      "Epoch [10/1], Step [150/1875], Loss: 0.5833\n",
      "Epoch [10/1], Step [200/1875], Loss: 0.1091\n",
      "Epoch [10/1], Step [250/1875], Loss: 0.1417\n",
      "Epoch [10/1], Step [300/1875], Loss: 0.0541\n",
      "Epoch [10/1], Step [350/1875], Loss: 0.0609\n",
      "Epoch [10/1], Step [400/1875], Loss: 0.1685\n",
      "Epoch [10/1], Step [450/1875], Loss: 0.2536\n",
      "Epoch [10/1], Step [500/1875], Loss: 0.2923\n",
      "Epoch [10/1], Step [550/1875], Loss: 0.2211\n",
      "Epoch [10/1], Step [600/1875], Loss: 0.3090\n",
      "Epoch [10/1], Step [650/1875], Loss: 0.1306\n",
      "Epoch [10/1], Step [700/1875], Loss: 0.1477\n",
      "Epoch [10/1], Step [750/1875], Loss: 0.3166\n",
      "Epoch [10/1], Step [800/1875], Loss: 0.3954\n",
      "Epoch [10/1], Step [850/1875], Loss: 0.1990\n",
      "Epoch [10/1], Step [900/1875], Loss: 0.0638\n",
      "Epoch [10/1], Step [950/1875], Loss: 0.1650\n",
      "Epoch [10/1], Step [1000/1875], Loss: 0.2750\n",
      "Epoch [10/1], Step [1050/1875], Loss: 0.3277\n",
      "Epoch [10/1], Step [1100/1875], Loss: 0.3294\n",
      "Epoch [10/1], Step [1150/1875], Loss: 0.1090\n",
      "Epoch [10/1], Step [1200/1875], Loss: 0.3256\n",
      "Epoch [10/1], Step [1250/1875], Loss: 0.2747\n",
      "Epoch [10/1], Step [1300/1875], Loss: 0.1923\n",
      "Epoch [10/1], Step [1350/1875], Loss: 0.0584\n",
      "Epoch [10/1], Step [1400/1875], Loss: 0.2052\n",
      "Epoch [10/1], Step [1450/1875], Loss: 0.1971\n",
      "Epoch [10/1], Step [1500/1875], Loss: 0.2598\n",
      "Epoch [10/1], Step [1550/1875], Loss: 0.1987\n",
      "Epoch [10/1], Step [1600/1875], Loss: 0.3468\n",
      "Epoch [10/1], Step [1650/1875], Loss: 0.0768\n",
      "Epoch [10/1], Step [1700/1875], Loss: 0.4074\n",
      "Epoch [10/1], Step [1750/1875], Loss: 0.1967\n",
      "Epoch [10/1], Step [1800/1875], Loss: 0.1479\n",
      "Epoch [10/1], Step [1850/1875], Loss: 0.2768\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "# Loss and optimizer\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "# Train the model\n",
    "total_step = len(train_loader)\n",
    "losses = []\n",
    "\n",
    "max_loss = float('inf')\n",
    "\n",
    "for epoch in range(10):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 50 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, 1, i+1, total_step, loss.item()))\n",
    "            if loss.item() < max_loss:\n",
    "                max_loss = loss.item()\n",
    "                torch.save(model.state_dict(), 'model.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('model.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy of the model on the 60000 test images: 92.76333333333334 %\n"
     ]
    }
   ],
   "source": [
    "model.eval()  # eval mode (batchnorm uses moving mean/variance instead of mini-batch mean/variance)\n",
    "with torch.no_grad():\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader:\n",
    "        images = images.to(device)\n",
    "        labels = labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "    acc = 100 * correct / total\n",
    "    print('Test Accuracy of the model on the 60000 test images: {} %'.format(acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = dict(model.fc.named_parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # To scale up the parameters\n",
    "# for pname in params:\n",
    "#     params[pname].data.copy_(params[pname].data * 100)\n",
    "\n",
    "# To quantize into ints\n",
    "for pname in params:\n",
    "    params[pname].data.copy_(params[pname].round())\n",
    "\n",
    "# To change into quantized values\n",
    "# for pname in params:\n",
    "#     w = params[pname]\n",
    "#     avg = torch.mean(torch.abs(w))\n",
    "#     sign = w.sign()\n",
    "#     params[pname].data.copy_(avg*sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'0.weight': Parameter containing:\n",
       " tensor([[ 777., -777.,  777.,  ..., -777., -777.,  777.],\n",
       "         [ 777., -777.,  777.,  ...,  777.,  777., -777.],\n",
       "         [-777.,  777., -777.,  ..., -777., -777.,  777.],\n",
       "         ...,\n",
       "         [ 777.,  777.,  777.,  ..., -777.,  777.,  777.],\n",
       "         [-777.,  777., -777.,  ..., -777.,  777.,  777.],\n",
       "         [ 777., -777., -777.,  ..., -777., -777.,  777.]],\n",
       "        device='cuda:0', requires_grad=True), '0.bias': Parameter containing:\n",
       " tensor([-188.,  188., -188.,  188., -188., -188., -188., -188., -188.,    0.,\n",
       "          188., -188.,  188., -188.,  188., -188.,  188., -188., -188., -188.,\n",
       "         -188., -188., -188., -188., -188.,  188., -188., -188.,  188.,  188.,\n",
       "          188.,  188., -188., -188., -188.,  188., -188.,  188., -188., -188.,\n",
       "         -188., -188.,  188., -188., -188., -188., -188., -188., -188., -188.,\n",
       "         -188., -188.,  188.,  188., -188.,  188.,  188., -188., -188.,  188.,\n",
       "         -188., -188.,  188., -188., -188., -188., -188., -188.,  188.,  188.,\n",
       "          188.,  188.,  188.,  188., -188., -188., -188.,  188.,  188., -188.,\n",
       "         -188., -188.,  188., -188., -188., -188., -188.,  188., -188., -188.,\n",
       "          188., -188., -188., -188.,  188.,  188., -188., -188., -188., -188.,\n",
       "         -188., -188., -188., -188.,  188., -188.,  188., -188.,  188., -188.,\n",
       "         -188., -188., -188.,  188., -188.,  188.,  188., -188., -188.,  188.,\n",
       "          188.,  188., -188.,  188.,  188.,  188., -188.,  188.],\n",
       "        device='cuda:0', requires_grad=True), '2.weight': Parameter containing:\n",
       " tensor([[-506., -506., -506.,  ...,  506., -506.,  506.],\n",
       "         [ 506., -506., -506.,  ...,  506., -506.,  506.],\n",
       "         [-506., -506.,  506.,  ..., -506.,  506., -506.],\n",
       "         ...,\n",
       "         [ 506.,  506.,  506.,  ...,  506., -506.,  506.],\n",
       "         [-506.,  506., -506.,  ...,  506., -506., -506.],\n",
       "         [ 506.,  506., -506.,  ...,  506., -506., -506.]],\n",
       "        device='cuda:0', requires_grad=True), '2.bias': Parameter containing:\n",
       " tensor([-11., -11.,  11., -11.,  11.,  11., -11.,  11., -11., -11., -11.,  11.,\n",
       "         -11.,  11., -11.,  11., -11.,  11.,  11., -11.,  11., -11.,  11., -11.,\n",
       "          11., -11.,  11., -11., -11., -11.,  11.,  11., -11.,  11.,  11.,  11.,\n",
       "          11., -11.,  11.,  11., -11.,  11.,  11., -11., -11., -11., -11.,  11.,\n",
       "          11.,  11., -11., -11., -11., -11., -11.,  11., -11., -11.,  11.,  11.,\n",
       "          11.,  11., -11., -11.], device='cuda:0', requires_grad=True), '4.weight': Parameter containing:\n",
       " tensor([[ 970.,  970., -970., -970., -970.,  970.,  970., -970.,  970., -970.,\n",
       "          -970., -970., -970.,  970., -970., -970.,  970., -970., -970., -970.,\n",
       "           970.,  970.,  970.,  970., -970.,  970.,  970.,  970.,  970.,  970.,\n",
       "          -970.,  970.,  970.,  970.,  970., -970.,  970., -970., -970., -970.,\n",
       "           970.,  970., -970., -970., -970.,  970., -970., -970.,  970.,  970.,\n",
       "          -970.,  970., -970., -970., -970., -970.,  970., -970., -970., -970.,\n",
       "           970., -970.,  970.,  970.],\n",
       "         [ 970.,  970., -970., -970.,  970., -970., -970.,  970.,  970.,  970.,\n",
       "          -970., -970., -970.,  970.,  970., -970.,  970.,  970., -970.,  970.,\n",
       "           970.,  970., -970.,  970., -970., -970., -970.,  970., -970., -970.,\n",
       "          -970.,  970.,  970., -970.,  970.,  970.,  970., -970., -970., -970.,\n",
       "          -970., -970.,  970., -970., -970., -970.,  970., -970., -970., -970.,\n",
       "           970.,  970., -970., -970.,  970.,  970.,  970.,  970., -970., -970.,\n",
       "          -970.,  970.,  970., -970.],\n",
       "         [-970., -970.,  970., -970.,  970.,  970.,  970., -970.,  970., -970.,\n",
       "           970.,  970.,  970., -970.,  970., -970., -970.,  970.,  970.,  970.,\n",
       "           970.,  970., -970., -970., -970., -970., -970., -970.,  970.,  970.,\n",
       "          -970.,  970., -970.,  970., -970., -970., -970.,  970., -970.,  970.,\n",
       "           970., -970.,  970.,  970., -970., -970., -970., -970., -970.,  970.,\n",
       "           970., -970., -970.,  970., -970.,  970.,  970.,  970.,  970., -970.,\n",
       "          -970., -970.,  970.,  970.],\n",
       "         [-970.,  970., -970.,  970.,  970., -970., -970.,    0., -970.,  970.,\n",
       "           970.,  970.,  970., -970.,  970.,  970.,  970., -970., -970.,  970.,\n",
       "          -970.,  970., -970., -970., -970.,  970., -970., -970., -970.,  970.,\n",
       "           970.,  970., -970.,  970., -970., -970., -970., -970., -970.,  970.,\n",
       "           970., -970., -970., -970., -970.,  970.,  970., -970., -970.,  970.,\n",
       "          -970., -970., -970.,  970., -970.,  970.,  970.,  970., -970.,  970.,\n",
       "          -970.,  970.,  970., -970.],\n",
       "         [-970., -970., -970., -970.,  970.,  970.,  970., -970., -970., -970.,\n",
       "           970.,  970.,  970., -970.,  970., -970., -970., -970., -970.,  970.,\n",
       "          -970., -970.,  970., -970.,  970., -970., -970.,  970., -970.,  970.,\n",
       "          -970., -970.,  970., -970., -970.,  970.,  970., -970.,  970.,  970.,\n",
       "           970.,  970.,  970.,  970.,  970., -970., -970.,  970.,  970., -970.,\n",
       "          -970., -970.,  970., -970.,  970., -970., -970.,  970.,  970.,  970.,\n",
       "          -970., -970.,  970.,  970.],\n",
       "         [ 970.,  970., -970.,  970., -970., -970., -970., -970., -970.,  970.,\n",
       "          -970.,  970., -970., -970., -970.,  970.,  970.,  970., -970., -970.,\n",
       "          -970.,  970., -970., -970.,  970.,  970., -970., -970.,  970., -970.,\n",
       "           970., -970., -970., -970.,  970., -970.,  970., -970.,  970., -970.,\n",
       "          -970.,  970.,  970.,  970., -970., -970., -970., -970., -970.,  970.,\n",
       "          -970.,  970.,  970.,  970., -970.,  970., -970.,  970.,  970.,  970.,\n",
       "           970.,  970.,  970., -970.],\n",
       "         [-970., -970., -970., -970., -970., -970.,  970., -970., -970., -970.,\n",
       "           970., -970., -970., -970., -970.,  970.,  970., -970.,  970.,  970.,\n",
       "           970.,  970., -970.,  970., -970., -970., -970., -970.,  970.,  970.,\n",
       "           970., -970.,  970., -970.,  970.,  970., -970.,  970.,  970., -970.,\n",
       "          -970., -970.,  970., -970., -970., -970.,  970., -970.,  970., -970.,\n",
       "          -970.,  970.,  970., -970., -970., -970., -970., -970.,  970., -970.,\n",
       "          -970.,  970.,  970., -970.],\n",
       "         [ 970.,  970., -970., -970.,  970., -970., -970.,  970.,  970., -970.,\n",
       "          -970., -970.,  970.,  970., -970.,  970., -970.,  970., -970., -970.,\n",
       "           970.,  970.,  970., -970., -970., -970.,  970., -970.,  970., -970.,\n",
       "          -970., -970., -970., -970., -970.,  970., -970., -970., -970., -970.,\n",
       "          -970., -970., -970., -970.,  970.,  970.,  970., -970.,  970., -970.,\n",
       "           970., -970.,  970.,  970., -970., -970.,  970., -970.,  970.,  970.,\n",
       "          -970.,  970., -970.,  970.],\n",
       "         [-970., -970.,  970.,  970.,  970., -970., -970.,  970., -970.,  970.,\n",
       "           970., -970.,  970., -970.,  970., -970., -970.,  970., -970.,  970.,\n",
       "          -970.,  970., -970.,  970.,  970., -970.,  970., -970., -970.,  970.,\n",
       "          -970.,  970.,  970., -970.,  970.,  970., -970.,  970., -970., -970.,\n",
       "           970.,  970., -970.,  970., -970.,  970., -970.,  970., -970.,  970.,\n",
       "          -970., -970.,  970., -970., -970.,  970., -970.,  970.,  970., -970.,\n",
       "           970.,  970.,  970., -970.],\n",
       "         [ 970.,  970.,  970.,  970., -970.,  970.,  970.,  970.,  970., -970.,\n",
       "          -970., -970., -970.,  970., -970.,  970., -970.,  970., -970.,  970.,\n",
       "          -970., -970.,  970., -970.,  970.,  970.,  970., -970., -970.,  970.,\n",
       "          -970., -970.,  970., -970.,  970.,  970., -970.,  970., -970., -970.,\n",
       "           970.,  970., -970., -970.,  970.,  970.,  970.,  970.,  970., -970.,\n",
       "           970., -970., -970., -970., -970.,  970.,  970.,  970.,  970., -970.,\n",
       "          -970.,  970., -970., -970.]], device='cuda:0', requires_grad=True), '4.bias': Parameter containing:\n",
       " tensor([-98.,  98., -98., -98., -98.,  98.,  98.,  98., -98., -98.],\n",
       "        device='cuda:0', requires_grad=True)}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "777.0 [[1 0 1 ... 0 0 1]\n",
      " [1 0 1 ... 1 1 0]\n",
      " [0 1 0 ... 0 0 1]\n",
      " ...\n",
      " [1 1 1 ... 0 1 1]\n",
      " [0 1 0 ... 0 1 1]\n",
      " [1 0 0 ... 0 0 1]]\n",
      "188.0 [0 1 0 1 0 0 0 0 0 0 1 0 1 0 1 0 1 0 0 0 0 0 0 0 0 1 0 0 1 1 1 1 0 0 0 1 0\n",
      " 1 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 1 1 0 0 1 0 0 1 0 0 0 0 0 1 1 1 1 1 1\n",
      " 0 0 0 1 1 0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 0 1 0 0\n",
      " 0 0 1 0 1 1 0 0 1 1 1 0 1 1 1 0 1]\n",
      "506.0 [[0 0 0 ... 1 0 1]\n",
      " [1 0 0 ... 1 0 1]\n",
      " [0 0 1 ... 0 1 0]\n",
      " ...\n",
      " [1 1 1 ... 1 0 1]\n",
      " [0 1 0 ... 1 0 0]\n",
      " [1 1 0 ... 1 0 0]]\n",
      "11.0 [0 0 1 0 1 1 0 1 0 0 0 1 0 1 0 1 0 1 1 0 1 0 1 0 1 0 1 0 0 0 1 1 0 1 1 1 1\n",
      " 0 1 1 0 1 1 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1 1 1 1 0 0]\n",
      "970.0 [[1 1 0 0 0 1 1 0 1 0 0 0 0 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1 0\n",
      "  1 0 0 0 1 1 0 0 0 1 0 0 1 1 0 1 0 0 0 0 1 0 0 0 1 0 1 1]\n",
      " [1 1 0 0 1 0 0 1 1 1 0 0 0 1 1 0 1 1 0 1 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 1\n",
      "  1 0 0 0 0 0 1 0 0 0 1 0 0 0 1 1 0 0 1 1 1 1 0 0 0 1 1 0]\n",
      " [0 0 1 0 1 1 1 0 1 0 1 1 1 0 1 0 0 1 1 1 1 1 0 0 0 0 0 0 1 1 0 1 0 1 0 0\n",
      "  0 1 0 1 1 0 1 1 0 0 0 0 0 1 1 0 0 1 0 1 1 1 1 0 0 0 1 1]\n",
      " [0 1 0 1 1 0 0 0 0 1 1 1 1 0 1 1 1 0 0 1 0 1 0 0 0 1 0 0 0 1 1 1 0 1 0 0\n",
      "  0 0 0 1 1 0 0 0 0 1 1 0 0 1 0 0 0 1 0 1 1 1 0 1 0 1 1 0]\n",
      " [0 0 0 0 1 1 1 0 0 0 1 1 1 0 1 0 0 0 0 1 0 0 1 0 1 0 0 1 0 1 0 0 1 0 0 1\n",
      "  1 0 1 1 1 1 1 1 1 0 0 1 1 0 0 0 1 0 1 0 0 1 1 1 0 0 1 1]\n",
      " [1 1 0 1 0 0 0 0 0 1 0 1 0 0 0 1 1 1 0 0 0 1 0 0 1 1 0 0 1 0 1 0 0 0 1 0\n",
      "  1 0 1 0 0 1 1 1 0 0 0 0 0 1 0 1 1 1 0 1 0 1 1 1 1 1 1 0]\n",
      " [0 0 0 0 0 0 1 0 0 0 1 0 0 0 0 1 1 0 1 1 1 1 0 1 0 0 0 0 1 1 1 0 1 0 1 1\n",
      "  0 1 1 0 0 0 1 0 0 0 1 0 1 0 0 1 1 0 0 0 0 0 1 0 0 1 1 0]\n",
      " [1 1 0 0 1 0 0 1 1 0 0 0 1 1 0 1 0 1 0 0 1 1 1 0 0 0 1 0 1 0 0 0 0 0 0 1\n",
      "  0 0 0 0 0 0 0 0 1 1 1 0 1 0 1 0 1 1 0 0 1 0 1 1 0 1 0 1]\n",
      " [0 0 1 1 1 0 0 1 0 1 1 0 1 0 1 0 0 1 0 1 0 1 0 1 1 0 1 0 0 1 0 1 1 0 1 1\n",
      "  0 1 0 0 1 1 0 1 0 1 0 1 0 1 0 0 1 0 0 1 0 1 1 0 1 1 1 0]\n",
      " [1 1 1 1 0 1 1 1 1 0 0 0 0 1 0 1 0 1 0 1 0 0 1 0 1 1 1 0 0 1 0 0 1 0 1 1\n",
      "  0 1 0 0 1 1 0 0 1 1 1 1 1 0 1 0 0 0 0 1 1 1 1 0 0 1 0 0]]\n",
      "98.0 [0 1 0 0 0 1 1 1 0 0]\n"
     ]
    }
   ],
   "source": [
    "# To actually retrieve the values, we take the magnitude and the signs:\n",
    "for pname in params:\n",
    "    w = params[pname].data.cpu().numpy()\n",
    "    mag = np.amax(w)\n",
    "    sign = np.sign(w)\n",
    "    sign[sign < 0] = 0\n",
    "    sign = sign.astype(np.int8)\n",
    "    print(mag, sign)\n",
    "    np.save('{}__{}'.format(pname, int(mag)), sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sanity check\n",
    "b0 = np.load('0.bias__188.npy')\n",
    "w0 = np.load('0.weight__777.npy').astype(int)\n",
    "b2 = np.load('2.bias__11.npy')\n",
    "w2 = np.load('2.weight__506.npy').astype(int)\n",
    "b4 = np.load('4.bias__98.npy')\n",
    "w4 = np.load('4.weight__970.npy').astype(int)\n",
    "\n",
    "w0[w0 == 0] = -1\n",
    "w2[w2 == 0] = -1\n",
    "w4[w4 == 0] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = X.astype(int)\n",
    "x = (x.dot(w0.T) * 777) + (b0 * 188)\n",
    "x[x < 0] = 0\n",
    "x = np.sign(x) * int(np.mean(np.abs(x)))\n",
    "x = (x.dot(w2.T) * 506) + (b2 * 11)\n",
    "x[x < 0] = 0\n",
    "x = np.sign(x) * int(np.mean(np.abs(x)))\n",
    "x = (x.dot(w4.T) * 970) + (b4 * 98)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype('int64')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 0, 4, ..., 5, 6, 8])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(np.argmax(x, axis=1) == y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
